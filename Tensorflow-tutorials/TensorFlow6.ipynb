{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making New Layers and Models via Subclassing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Objectives:\n",
    "- Use Layer class as the combination of state (weights) and computation\n",
    "- Defer weight creation until the shape of the inputs is known\n",
    "- Build recursively compostable layers\n",
    "- Compute loss using add_loss() method\n",
    "- Compute average using add_metric() method\n",
    "- Enable serialization on layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Layer calss: the combination of state (weigths) and some computation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Densely connected layer. It has variables w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w)+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.06749453 -0.05709337  0.01190891  0.03545174]\n",
      " [-0.06749453 -0.05709337  0.01190891  0.03545174]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.1557091  -0.06040075  0.01923282 -0.01216238]\n",
      " [ 0.1557091  -0.06040075  0.01923282 -0.01216238]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(units,), initializer=\"zeros\", trainable=True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers can have non-trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim),),trainable=False)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 1\n",
      "non-trainable weights: 1\n",
      "trainable_weights: []\n"
     ]
    }
   ],
   "source": [
    "print(\"weights:\", len(my_sum.weights))\n",
    "print(\"non-trainable weights:\", len(my_sum.non_trainable_weights))\n",
    "print(\"trainable_weights:\", my_sum.trainable_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practice: deferring weight creation until the shape of the inputs is known"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the input_dim was used to compute the shape of the weights w and b in __init__()\n",
    "\n",
    "In many cases, we do not know in advanced the size of the inputs. We would want to create the weights after instantiating the layer.\n",
    "\n",
    "Create layer weights in the build(self, input_shape) method layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        print(\"init_method\")\n",
    "        super(Linear,self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        print(\"build method\")\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        print(\"call method\")\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_method\n"
     ]
    }
   ],
   "source": [
    "linear_Layer = Linear(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build method\n",
      "call method\n"
     ]
    }
   ],
   "source": [
    "y = linear_Layer(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers are recursively composable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you assign a layer instance as an attribute of another Layer, the outer layer will start tracking the weights of the inner layer.\n",
    "\n",
    "We recommend creating such sublayers in the __init__() method (since the sublayers will typucally have build method, they will be built when the outer layer gets built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_method\n",
      "init_method\n",
      "init_method\n",
      "build method\n",
      "call method\n",
      "build method\n",
      "call method\n",
      "build method\n",
      "call method\n",
      "weights: 6\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3,64)))\n",
    "print(\"weights:\", len(mlp.weights))\n",
    "print(\"trainable weights:\", len(mlp.trainable_weights))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The add_loss() method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When writing the call() method of a layer, you can create loss tensors that you will want to use later, when writitng your training loop.  \n",
    "\n",
    "This is doable by calling self.add_loss(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate*tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These losses (including thosee by any inner layer) can be retrived via layer.losses. This property is reset at the start of every __call__() to the tip-level layer, so that ayer.losses always contains the loss values created during the last forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OuterLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0\n",
    "\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1\n",
    "\n",
    "\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0021020933>]\n"
     ]
    }
   ],
   "source": [
    "class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayerWithKernelRegularizer, self).__init__()\n",
    "        self.dense = keras.layers.Dense(\n",
    "            32, kernel_regularizer=tf.keras.regularizers.l2(1e-3)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayerWithKernelRegularizer()\n",
    "_ = layer(tf.zeros((1, 1)))\n",
    "print(layer.losses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These losses work seamlessly with fit() (they get automatically summed and added to the main loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 399ms/step - loss: 0.2657\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.0169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25d49c37f70>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "outputs = ActivityRegularizationLayer()(inputs)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(np.random.random((2,3)), np.random.random((2,3)))\n",
    "\n",
    "model.compile(optimizer=\"adam\")\n",
    "model.fit(np.random.random((2,3)), np.random.random((2,3)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The add_metric() method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers have an add_metric() method for tracking the moving average of a quantity during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "    \n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_metric(acc, name='accuracy')\n",
    "\n",
    "        return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.metrics [<keras.metrics.accuracy_metrics.BinaryAccuracy object at 0x0000025D48782760>]\n",
      "current accuracy value 1.0\n"
     ]
    }
   ],
   "source": [
    "layer = LogisticEndpoint()\n",
    "\n",
    "targets = tf.ones((2,2))\n",
    "logits = tf.ones((2,2))\n",
    "y = layer(targets, logits)\n",
    "\n",
    "print(\"layer.metrics\", layer.metrics)\n",
    "print(\"current accuracy value\", float(layer.metrics[0].result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 1.0333 - binary_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25d4acefb80>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
    "\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "model.compile(optimizer=\"adam\")\n",
    "\n",
    "data = {\n",
    "    \"inputs\": np.random.random((3, 3)),\n",
    "    \"targets\": np.random.random((3, 10)),\n",
    "}\n",
    "model.fit(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can optionally enable serialization on your layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For layers to be serializable as a part of a functional model, implement get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"units\": self.units} \n",
    "\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'linear_9', 'trainable': True, 'dtype': 'float32', 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n",
    "\n",
    "\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privileged training argument in the call() method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some layers such as BatchNormalization and Dropout layers have different behaviours during tainning and inference.\n",
    "\n",
    "It is standart practive to expose a trainning argument in the call method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropout(keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privileged mask argument in the call() method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RNN layers, a mask is a boolean tensor (one bollean value per timestamp in the input) used to skip input timesteps when processing timeseries data.\n",
    "\n",
    "Keras will automatically pass the correct mask argument to __call__() for layers that support it, when a mask is generated by a prior layer. Mask-generating layers are the Embedding layer configured with mask_zero=True, and the Masking layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together: an end-to-end example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A layer encapsulates a state (crated in init or build) and some computation (defined in call)\n",
    "- Layers can be recursivaly nested to create new, bigger computation blocks.\n",
    "- Layers can create and tarck losses as well as metrics (vai add_loss() and add_metrics())\n",
    "- The outer container, the thing you want to train is a Model. A Model is like a layer but with added trainning and serialization utilities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim  = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermidiate_dim=64, name='encoder', **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermidiate_dim, activation='relu')\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "    \n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, original_dim, intermidiate_dim=64, name='decoder', **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermidiate_dim, activation='relu')\n",
    "        self.dense_output = layers.Dense(original_dim, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "    \n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=64,\n",
    "        latent_dim=32,\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermidiate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermidiate_dim=intermediate_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        kl_loss =  -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = 0.3556\n",
      "step 100: mean loss = 0.1250\n",
      "step 200: mean loss = 0.0989\n",
      "step 300: mean loss = 0.0890\n",
      "step 400: mean loss = 0.0841\n",
      "step 500: mean loss = 0.0808\n",
      "step 600: mean loss = 0.0787\n",
      "step 700: mean loss = 0.0771\n",
      "step 800: mean loss = 0.0759\n",
      "step 900: mean loss = 0.0749\n",
      "Start of epoch 1\n",
      "step 0: mean loss = 0.0746\n",
      "step 100: mean loss = 0.0740\n",
      "step 200: mean loss = 0.0735\n",
      "step 300: mean loss = 0.0730\n",
      "step 400: mean loss = 0.0727\n",
      "step 500: mean loss = 0.0723\n",
      "step 600: mean loss = 0.0720\n",
      "step 700: mean loss = 0.0717\n",
      "step 800: mean loss = 0.0714\n",
      "step 900: mean loss = 0.0712\n"
     ]
    }
   ],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, intermediate_dim=64, latent_dim=32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _  = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\")/255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" %(epoch,))\n",
    "\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)\n",
    "        \n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step %100 ==0:\n",
    "            print(\"step %d: mean loss = %.4f\" %(step,loss_metric.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "938/938 [==============================] - 26s 6ms/step - loss: 0.0746\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 6s 6ms/step - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2973d7c9fa0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
